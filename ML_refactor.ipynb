{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer as SIA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"Reviews_withURL.csv\")\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SIA()\n",
    "df[\"Sentiment\"] = df[\"Text\"].apply(lambda x: 1 if sia.polarity_scores(x)[\"compound\"] > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_pos_tagging(text):\n",
    "    \n",
    "    text = re.sub(\"[0-9]|br|<|>|com|<br>\", \"\", text, 0, re.MULTILINE)\n",
    "    words = text.split()\n",
    "    \n",
    "    return nltk.tag.pos_tag(words)\n",
    "\n",
    "df['TextSegment'] = df.Text.apply(segment_pos_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconnect(text_segment):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "    reconnect_adj = []\n",
    "    \n",
    "    for i in range(len(text_segment)):\n",
    "        if text_segment[i][1] in pos_tags:\n",
    "            adj = text_segment[i][0]\n",
    "            adj = adj.replace(' ', '').replace(',','').replace(' ','').replace('/', '').replace('_', '')\n",
    "            # adj = lemmatizer.lemmatize(adj, pos=\"a\")\n",
    "            if text_segment[i][1] == 'JJ':\n",
    "                adj = lemmatizer.lemmatize(adj, pos=\"v\")\n",
    "            elif text_segment[i][1] == 'JJR':\n",
    "                adj = lemmatizer.lemmatize(adj, pos=\"a\")\n",
    "            elif text_segment[i][1] == 'JJS':\n",
    "                adj = lemmatizer.lemmatize(adj, pos=\"a\")\n",
    "            if len(adj) >= 4:\n",
    "\n",
    "                if text_segment[i-1][0] == 'not':\n",
    "                    reconnect_adj.append(f'not_{adj}')\n",
    "                \n",
    "                else:\n",
    "                    reconnect_adj.append(adj)\n",
    "            \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    return \" \".join(text for text in reconnect_adj)\n",
    "\n",
    "df['TextAdj'] = df.TextSegment.apply(reconnect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TextAdj', 'Sentiment', 'HelpfulnessNumerator', 'HelpfulnessDenominator']].to_csv(r\"Reviews_withURL_preprocessing_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"Reviews_withURL_preprocessing.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"Reviews_withURL_preprocessing_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "# df['TextAdj'] = df['TextAdj'].apply(lambda x :[stemmer.stem(i) for i in str(x).split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['TextAdj'] = df['TextAdj'].apply(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"HelpfulnessDenominator\"] > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"HelpfulnessDenominator\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"HelpfulnessNumerator\"] < df[\"HelpfulnessDenominator\"]]\n",
    "df[\"HelpfulnessRatio\"] = df[\"HelpfulnessNumerator\"] / df[\"HelpfulnessDenominator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_helpfulness_class(row):\n",
    "    threshold = 0.65\n",
    "    # if row[\"HelpfulnessRatio\"] > threshold:\n",
    "    #     return \"Helpful\"\n",
    "    # else:\n",
    "    #     return \"Unhelpful\"\n",
    "    if row[\"HelpfulnessRatio\"] > threshold and row[\"Sentiment\"] == 1:\n",
    "        return \"HelpfulPos\"\n",
    "    elif row[\"HelpfulnessRatio\"] > threshold and row[\"Sentiment\"] == -1:\n",
    "        return \"HelpfulNeg\"\n",
    "    elif row[\"HelpfulnessRatio\"] <= threshold and row[\"Sentiment\"] == 1:\n",
    "        return \"UnhelpfulPos\"\n",
    "    elif row[\"HelpfulnessRatio\"] <= threshold and row[\"Sentiment\"] == -1:\n",
    "        return \"UnhelpfulNeg\"\n",
    "    \n",
    "def to_emotion_class(row):\n",
    "    if row[\"Sentiment\"] == 1:\n",
    "        return \"Pos\"\n",
    "    elif row[\"Sentiment\"] == -1:\n",
    "        return \"Neg\"\n",
    "\n",
    "df['HelpfulnessClass'] = df.apply(to_helpfulness_class, axis=1)\n",
    "df['EmotionClass'] = df.apply(to_emotion_class, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HelpfulPos      24502\n",
       "UnhelpfulPos    15748\n",
       "UnhelpfulNeg     8671\n",
       "HelpfulNeg       5426\n",
       "Name: HelpfulnessClass, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['HelpfulnessClass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pos    40250\n",
       "Neg    14097\n",
       "Name: EmotionClass, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['EmotionClass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as TTS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = df.TextAdj\n",
    "y = df.HelpfulnessClass\n",
    "lb = LabelEncoder()\n",
    "y = lb.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = TTS(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), min_df=0.0001)\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 1), min_df=0.0001, max_df=0.99)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # vectorizer.get_feature_names_out().shape\n",
    "# from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# cc = ClusterCentroids(random_state=0)\n",
    "# X_train_resampled, y_train_resampled = cc.fit_resample(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "model_name = '4class.sav'\n",
    "pickle.dump(rfc, open(model_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = rfc.predict(X_test_tfidf)\n",
    "\n",
    "model_name = '4class.sav'\n",
    "\n",
    "model = pickle.load(open(model_name, 'rb'))\n",
    "y_pred = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6620055197792089\n",
      "Confusion Matrix: \n",
      " [[ 594  547  235  248]\n",
      " [  53 6460  232  641]\n",
      " [  84  622 1361  490]\n",
      " [  67 2026  266 2379]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  HelpfulNeg       0.74      0.37      0.49      1624\n",
      "  HelpfulPos       0.67      0.87      0.76      7386\n",
      "UnhelpfulNeg       0.65      0.53      0.59      2557\n",
      "UnhelpfulPos       0.63      0.50      0.56      4738\n",
      "\n",
      "    accuracy                           0.66     16305\n",
      "   macro avg       0.67      0.57      0.60     16305\n",
      "weighted avg       0.66      0.66      0.65     16305\n",
      "\n",
      "Recall Score:  0.6620055197792089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Recall Score: \", recall_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6554431156087089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "# 建立基礎分類器\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('xgb', XGBClassifier())\n",
    "]\n",
    "\n",
    "# 建立堆疊分類器\n",
    "stacking_classifier = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RandomForestClassifier(random_state=42)\n",
    ")\n",
    "\n",
    "# 訓練堆疊分類器\n",
    "stacking_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 在測試集上進行預測\n",
    "y_pred = stacking_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# 評估預測結果\n",
    "accuracy = stacking_classifier.score(X_test_tfidf, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = TTS(df[\"Text\"], df[\"Helpfulness\"], test_size=0.3, random_state=42)\n",
    "\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     min_df=0.0001,\n",
    "#     # ngram_range=(1, 1), \n",
    "#     # stop_words='english'\n",
    "# )\n",
    "# vectorizer.fit(df[\"TextAdj\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in vectorizer.get_feature_names_out():\n",
    "#     print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
